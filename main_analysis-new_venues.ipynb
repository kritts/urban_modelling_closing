{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "no such file or directory: '../data/MP14_SUBZONE_WEB_PL.shp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f307e8398b2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# data.london.gov.uk/dataset/statistical-gis-boundary-files-london\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0msingapore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/MP14_SUBZONE_WEB_PL.shp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;31m# from the prj file: http://www.prj2epsg.org/epsg/27700\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0msingapore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msingapore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_crs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'init'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'epsg:4277'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/geopandas/io/file.pyc\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(filename, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \"\"\"\n\u001b[1;32m     20\u001b[0m     \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bbox'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mfiona\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mcrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/fiona/__init__.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"no such archive file: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0marchive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"no such file or directory: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         c = Collection(path, mode, driver=driver, encoding=encoding,\n\u001b[1;32m    168\u001b[0m                        \u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvsi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvsi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marchive\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: no such file or directory: '../data/MP14_SUBZONE_WEB_PL.shp'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from dit.divergences import jensen_shannon_divergence\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot as plt\n",
    "from dateutil.parser import parse\n",
    "import plotly.graph_objs as go\n",
    "from shapely.geometry import *\n",
    "from datetime import datetime  \n",
    "import plotly.plotly as py\n",
    "import scipy.stats as st\n",
    "import geopandas as gpd\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import operator\n",
    "import geojson\n",
    "import math \n",
    "import sys \n",
    "import dit\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# data.london.gov.uk/dataset/statistical-gis-boundary-files-london \n",
    "singapore = gpd.read_file('../data/MP14_SUBZONE_WEB_PL.shp')\n",
    "# from the prj file: http://www.prj2epsg.org/epsg/27700\n",
    "singapore = singapore.to_crs({'init': 'epsg:4277'})   \n",
    "\n",
    "# path to data\n",
    "venues = \"../../../raw_data/venues/Singapore_venues.txt\"\n",
    "transitions = \"../../../raw_data/transitions/Singapore_transitions.txt\"\n",
    "\n",
    "# venue ID -> name\n",
    "venue_id_to_name = {} \n",
    "# venue ID -> coordinates for that venue\n",
    "venue_id_to_coords = {} \n",
    "# venue ID -> SPECIFIC category\n",
    "venue_id_to_category_specific = {} \n",
    "# venue ID -> GENERAL category\n",
    "venue_id_to_category_general = {} \n",
    "# venue ID -> opening date\n",
    "venue_id_to_opening = {}  \n",
    "\n",
    "### STEP 1: get all venues \n",
    "with open(venues, 'r') as f:\n",
    "    for rows in f:\n",
    "        rows = rows.split(\"\\t\")\n",
    "        # unique ID \n",
    "        ID = rows[0] \n",
    "        # name\n",
    "        name = rows[1]\n",
    "        venue_id_to_name[ID] = name \n",
    "        # coordinates \n",
    "        coordinates = (float(rows[2]), float(rows[3]))\n",
    "        venue_id_to_coords[ID] = coordinates  \n",
    "        # specific category\n",
    "        type_venue = rows[7]\n",
    "        venue_id_to_category_specific[ID] = type_venue  \n",
    "        # general categorgy\n",
    "        type_venue = rows[8]\n",
    "        venue_id_to_category_general[ID] = type_venue  \n",
    "        \n",
    "        # date\n",
    "        date = rows[9].strip()  \n",
    "        date_cleaned = datetime.strptime(date, '%Y-%m-%d')\n",
    "        venue_id_to_opening[ID] = date_cleaned\n",
    "        \n",
    "\n",
    "LIST_OF_VENUES = venue_id_to_opening.keys()\n",
    "\n",
    "# venue ID -> list of *FIRST* checkins to that venue       \n",
    "venue_id_to_times = {} \n",
    "with open(transitions, 'r') as f:\n",
    "    for rows in f:\n",
    "        rows = rows.split(\"\\t\")\n",
    "        location1 = rows[0]\n",
    "        time1 = rows[1] \n",
    "        if location1 in venue_id_to_coords:\n",
    "            venue_id_to_times.setdefault(location1, [])\n",
    "            time_cleaned = datetime.fromtimestamp(int(time1))\n",
    "            venue_id_to_times[location1].append(time_cleaned)\n",
    "             \n",
    "# venue id to the number of checkins to that venue \n",
    "venue_id_to_num = {}\n",
    "for v in LIST_OF_VENUES:\n",
    "    if v not in venue_id_to_times:\n",
    "        continue \n",
    "    venue_id_to_num[v] = len(venue_id_to_times[v])    \n",
    "    \n",
    "# venue ID -> ward\n",
    "venue_id_to_ward = {}\n",
    "for venue in LIST_OF_VENUES: \n",
    "    coordinates = venue_id_to_coords[venue]\n",
    "    latitude, longitude = coordinates\n",
    "    point_of_checkin = Point(longitude, latitude) \n",
    "    \n",
    "    does_contain_point = singapore.contains(point_of_checkin) \n",
    "    contains = does_contain_point[does_contain_point == True]      \n",
    "    if len(contains) > 0:\n",
    "        ward = contains.index[0] \n",
    "        venue_id_to_ward[venue] = ward \n",
    "\n",
    "# ward to set of venues in that ward\n",
    "ward_to_set_venues = {} \n",
    "# venue id to the number of checkins to that venue \n",
    "venue_id_to_num = {}\n",
    "for v in LIST_OF_VENUES:\n",
    "    if v not in venue_id_to_times:\n",
    "        continue \n",
    "    venue_id_to_num[v] = len(venue_id_to_times[v])\n",
    "    \n",
    "    if v not in venue_id_to_ward: \n",
    "        continue \n",
    "    ward = venue_id_to_ward[v]\n",
    "    ward_to_set_venues.setdefault(ward, set())\n",
    "    ward_to_set_venues[ward].add(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### STEP 2: get all new venues\n",
    "# get a set of places that opened after the day we've selected  \n",
    "places_opened_to_num = {}\n",
    "for v in venue_id_to_opening: \n",
    "    if v not in venue_id_to_num: \n",
    "        continue \n",
    "    date = venue_id_to_opening[v]\n",
    "    year = date.year\n",
    "    month = date.month\n",
    "    if (year > 2011 and month > 6) or year > 2012: \n",
    "        places_opened_to_num[v] = venue_id_to_num[v]\n",
    "          \n",
    "# list of new venues that have at least 100 checkins \n",
    "venues_above_threshold = [venue for venue in places_opened_to_num if places_opened_to_num[venue] > 100]\n",
    "print len(venues_above_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check if there's data last 6 months\n",
    "# is there less than 12 checkins after july 2013\n",
    "venues_closed = set()\n",
    "# whether venue is open or closed \n",
    "venue_to_status = {}\n",
    "for v in venues_above_threshold:\n",
    "    if v not in venue_id_to_times: \n",
    "        continue\n",
    "    times = venue_id_to_times[v]\n",
    "    checkins_after = 0\n",
    "    for date in times:\n",
    "        year = date.year\n",
    "        month = date.month\n",
    "        if year == 2013 and month > 6 or year > 2014:\n",
    "            checkins_after += 1 \n",
    "    if checkins_after < 50: # less than 5 checkins a month\n",
    "        venues_closed.add(v)     \n",
    "        venue_to_status[v] = 0\n",
    "    else: \n",
    "        venue_to_status[v] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "761 days, 0:00:00\n",
      "94\n"
     ]
    }
   ],
   "source": [
    "num_closed = 0\n",
    "\n",
    "date_comparison = datetime.strptime('Jul2013', '%b%Y')\n",
    "max_possible = datetime.strptime('Jun2011', '%b%Y')   \n",
    "max_possible = date_comparison - max_possible\n",
    "arr_days_open = []\n",
    "\n",
    "for v in venues_above_threshold: \n",
    "    if venue_to_status[v] == 0:\n",
    "        num_closed += 1 \n",
    "        delta = date_comparison - venue_id_to_opening[v]\n",
    "        arr_days_open.append(delta.days)\n",
    "        #print venue_id_to_opening[v], \" - \", date_comparison, \" - \", delta \n",
    "    \n",
    "print max_possible\n",
    "print len(arr_days_open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# when people end up at this ward, from where do they come?\n",
    "# that same ward or a different ward?\n",
    "ward_to_num_inflow_transitions = {}\n",
    "ward_to_num_within_transitions = {}\n",
    "\n",
    "# when people leave this ward, where do they go? \n",
    "# that same ward or a different ward?\n",
    "ward_to_num_outflow_transitions = {} \n",
    "error_counter = 0\n",
    "with open(transitions, 'r') as f:\n",
    "    for rows in f:\n",
    "        rows = rows.split(\"\\t\")\n",
    "        location_1 = rows[0]\n",
    "        location_2 = rows[2]\n",
    "        if location_1 not in venue_id_to_ward or location_2 not in venue_id_to_ward: \n",
    "            error_counter += 1\n",
    "            continue\n",
    "        ward_1 = venue_id_to_ward[location_1]\n",
    "        ward_2 = venue_id_to_ward[location_2]\n",
    "        if ward_1 == ward_2:\n",
    "            ward_to_num_within_transitions.setdefault(ward_2, 0)\n",
    "            ward_to_num_within_transitions[ward_2] += 1\n",
    "        else:  \n",
    "            ward_to_num_inflow_transitions.setdefault(ward_2, 0)\n",
    "            ward_to_num_inflow_transitions[ward_2] += 1 \n",
    "            \n",
    "            ward_to_num_outflow_transitions.setdefault(ward_1, 0)\n",
    "            ward_to_num_outflow_transitions[ward_1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_general = list(set(venue_id_to_category_general.values()))\n",
    "all_specific = list(set(venue_id_to_category_specific.values()))\n",
    "\n",
    "myData = np.array([[\"venue\", \"ward\", \"within\", \"in\", \"out\", \"ratio in\", \"ratio out\", \"ratio in/out\", \"spec\", \"gen\", \"closed or open\"]])\n",
    "NUM_FEATURES = 10\n",
    "count = 0\n",
    "for new_venue in venues_above_threshold: \n",
    "    # travel out of city bounds \n",
    "    if new_venue not in venue_id_to_ward:  \n",
    "        continue \n",
    "    inner= np.array([])\n",
    "    new_ward = venue_id_to_ward[new_venue] \n",
    "    if new_ward not in ward_to_num_within_transitions or new_ward not in ward_to_num_inflow_transitions:  \n",
    "        print new_ward\n",
    "        continue\n",
    "    if new_ward not in ward_to_num_outflow_transitions or new_venue not in venue_to_status: \n",
    "        continue \n",
    "    if new_venue not in venue_id_to_category_specific or new_venue not in venue_id_to_category_general: \n",
    "        continue\n",
    "         \n",
    "    within = ward_to_num_within_transitions[new_ward]\n",
    "    inflow = ward_to_num_inflow_transitions[new_ward]\n",
    "    outflow = ward_to_num_outflow_transitions[new_ward]\n",
    "    status = venue_to_status[new_venue]\n",
    "    spec = venue_id_to_category_specific[new_venue]\n",
    "    gen = venue_id_to_category_general[new_venue]\n",
    "    index_spec = all_specific.index(spec)\n",
    "    index_gen = all_general.index(gen) \n",
    "    \n",
    "    \n",
    "    inner = np.append(inner, count)\n",
    "    inner = np.append(inner, new_ward)\n",
    "    inner = np.append(inner, within)\n",
    "    inner = np.append(inner, inflow)\n",
    "    inner = np.append(inner, outflow)    \n",
    "    inner = np.append(inner, float(within) / inflow)\n",
    "    inner = np.append(inner, float(within) / outflow)\n",
    "    inner = np.append(inner, float(inflow) / outflow)\n",
    "    inner = np.append(inner, index_spec)\n",
    "    inner = np.append(inner, index_gen)\n",
    "    \n",
    "    # output\n",
    "    inner = np.append(inner, status)  \n",
    "    myData = np.concatenate([myData, [inner]])   \n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing complete\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "myFile = open('features.csv', 'w')\n",
    "with myFile:\n",
    "    writer = csv.writer(myFile)\n",
    "    writer.writerows(myData)  \n",
    "print(\"Writing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 70.54% (14.52%)\n"
     ]
    }
   ],
   "source": [
    "dataframe = pandas.read_csv(\"features.csv\", header=None)\n",
    "dataset = dataframe.values \n",
    "\n",
    "X = dataset[:, 0:NUM_FEATURES]\n",
    "Y = dataset[:, NUM_FEATURES]\n",
    "\n",
    "def create_baseline(): \n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim=NUM_FEATURES, kernel_initializer='normal', activation='relu')) # intially 60\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid')) \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "    return model  \n",
    "\n",
    "estimator = KerasClassifier(build_fn=create_baseline, nb_epoch=100, batch_size=5, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "print(\"Results: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# step 3: train model\n",
    "# https://www.tensorflow.org/get_started/get_started\n",
    "# https://keras.io/\n",
    "# https://stackoverflow.com/questions/42532386/how-to-work-with-multiple-inputs-for-lstm-in-keras\n",
    "# https://github.com/titu1994/keras-SRU/blob/master/imdb_sru.py\n",
    "\n",
    "# https://machinelearningmastery.com/binary-classification-tutorial-with-the-keras-deep-learning-library/\n",
    "# https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/\n",
    "\n",
    "\n",
    "\n",
    "# MAIN\n",
    "# https://machinelearningmastery.com/binary-classification-tutorial-with-the-keras-deep-learning-library/\n",
    "\n",
    "# \n",
    "# https://keras.io/layers/core/\n",
    "\n",
    "# 30 - Results: 70.53% (28.97%) \n",
    "# 10 - Results: 78.88% (21.96%)\n",
    "# 1 - Results: 86.51% (0.78%)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
